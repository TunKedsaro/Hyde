{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Docstring for 2501291219-05.ipynb\n",
    "This notebook integrate between pipeline-01 and BigQuery input\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46343cfc-da21-4418-9f89-7a73f02d2e79",
   "metadata": {},
   "source": [
    "### Import important library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1af320d-7812-4db1-97a0-892c33a1e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885cb14d-f1a2-414a-9bfe-ac2c33226a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.utils.logging import get_logger\n",
    "from functions.utils.config  import PROJECT_ROOT, load_config\n",
    "from functions.utils.llm_client import build_llm_client_from_yaml\n",
    "from functions.utils.text_embeddings import GoogleEmbeddingModel\n",
    "from functions.core.context_builder import build_user_context\n",
    "from functions.core.history import build_history_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b3e92-30bb-4049-8e13-a4fa696592cf",
   "metadata": {},
   "source": [
    "### QueryData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ea0649-b82f-4b7a-9ffe-09c710b6e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e128a38b-e925-4314-a5ca-b4bbd4b1708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQuery:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client()\n",
    "    def get_students(self):\n",
    "        query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM `poc-piloturl-nonprod.gold_layer.students`\n",
    "        \"\"\"\n",
    "        df = self.client.query(query).to_dataframe()\n",
    "        return df\n",
    "    def get_interactions(self):\n",
    "        query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM `poc-piloturl-nonprod.gold_layer.interactions`\n",
    "        \"\"\"\n",
    "        df = self.client.query(query).to_dataframe()\n",
    "        return df \n",
    "    def get_user_events_json(self):\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `poc-piloturl-nonprod.gold_layer.feeds`\n",
    "        \"\"\"\n",
    "        df = self.client.query(query).to_dataframe()\n",
    "        # ensure created_at is ISO-8601 Z format\n",
    "        df[\"created_at\"] = df[\"created_at\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        feeds_lookup: Dict[str, Dict[str, Any]] = {}\n",
    "        for _,row in df.iterrows():\n",
    "            feed_id = row[\"feed_id\"]\n",
    "            feeds_lookup[feed_id] = {\n",
    "                \"feed_id\"        : feed_id,\n",
    "                \"title\"          : row[\"title\"],\n",
    "                \"feed_text\"      : row[\"feed_text\"],\n",
    "                \"tags\"           : row[\"tags\"],                     \n",
    "                \"language\"       : row[\"language\"],\n",
    "                \"created_at\"     : row[\"created_at\"],\n",
    "                \"source\"         : row[\"source\"],\n",
    "                \"url\"            : row[\"url\"],\n",
    "                \"views\"          : int(row[\"views\"]),\n",
    "                \"embedding_input\": row[\"embedding_input\"]\n",
    "            }\n",
    "        return feeds_lookup\n",
    "# dq = DataQuery()\n",
    "# dq.get_students()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7f890-bc57-48c4-8fb6-8f2f952fe227",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5749c14-0f25-4fcd-b208-d7b5cddae867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str) -> None:\n",
    "    \"\"\"Create directory if it does not exist (idempotent).\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "def _read_hyde_config(cfg: Dict[str, Any]) -> Tuple[int, int, int, bool, str]:\n",
    "    \"\"\"\n",
    "    Read HyDE-related configuration with safe defaults.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history_threshold:\n",
    "        Event count threshold for prompt selection\n",
    "    recent_k:\n",
    "        Max number of recent feeds used in HistorySummary\n",
    "    feed_text_max_chars:\n",
    "        Per-feed text truncation limit\n",
    "    include_recent_feeds:\n",
    "        Whether HistorySummary may include feed snippets\n",
    "    query_embedding_model_name:\n",
    "        Embedding model for HyDE queries\n",
    "    \"\"\"\n",
    "    hyde_cfg = cfg.get(\"hyde\", {}) if isinstance(cfg, dict) else {}\n",
    "\n",
    "    history_threshold = int(hyde_cfg.get(\"history_threshold\", 5))\n",
    "    recent_k = int(hyde_cfg.get(\"recent_k\", 5))\n",
    "    feed_text_max_chars = int(hyde_cfg.get(\"feed_text_max_chars\", 240))\n",
    "    include_recent_feeds = bool(hyde_cfg.get(\"include_recent_feeds\", True))\n",
    "\n",
    "    # Default to same embedding family as feed embeddings\n",
    "    query_embedding_model_name = str(\n",
    "        hyde_cfg.get(\"query_embedding_model_name\")\n",
    "        or cfg.get(\"embeddings\", {}).get(\"model_name\", \"\")\n",
    "        or \"gemini-embedding-001\"\n",
    "    )\n",
    "\n",
    "    # Hard safety guards\n",
    "    history_threshold = max(1, history_threshold)\n",
    "    recent_k = max(0, min(recent_k, 10))\n",
    "    feed_text_max_chars = max(0, min(feed_text_max_chars, 2000))\n",
    "\n",
    "    return (\n",
    "        history_threshold,\n",
    "        recent_k,\n",
    "        feed_text_max_chars,\n",
    "        include_recent_feeds,\n",
    "        query_embedding_model_name,\n",
    "    )\n",
    "    \n",
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Deterministic JSONL reader.\n",
    "\n",
    "    Order is preserved, which is critical for any downstream alignment.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Invalid JSONL at line {line_no}: {e}\") from e\n",
    "    return rows\n",
    "\n",
    "def load_prompts() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load HyDE prompt templates from parameters/prompts.yaml.\n",
    "\n",
    "    Expected structure:\n",
    "      hyde_prompts:\n",
    "        hyde_a: \"...\"\n",
    "        hyde_b: \"...\"\n",
    "        hyde_c: \"...\"\n",
    "    \"\"\"\n",
    "    import yaml\n",
    "\n",
    "    prompts_path = PROJECT_ROOT / \"parameters\" / \"prompts.yaml\"\n",
    "    with prompts_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f) or {}\n",
    "\n",
    "    return data.get(\"hyde_prompts\", {}) or {}\n",
    "\n",
    "# =============================================================================\n",
    "# Prompt selection and rendering\n",
    "# =============================================================================\n",
    "def choose_hyde_prompt_key(num_events: int, history_threshold: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Select HyDE prompt variant based on interaction volume.\n",
    "\n",
    "    Rules\n",
    "    -----\n",
    "    - num_events >= history_threshold → history-heavy (hyde_b)\n",
    "    - num_events <= 1               → onboarding / sparse (hyde_c)\n",
    "    - otherwise                     → mixed (hyde_a)\n",
    "    \"\"\"\n",
    "    if num_events >= history_threshold:\n",
    "        return \"hyde_b\"\n",
    "    if num_events <= 1:\n",
    "        return \"hyde_c\"\n",
    "    return \"hyde_a\"\n",
    "\n",
    "\n",
    "def render_prompt(\n",
    "    template: str,\n",
    "    preferred_language: str,\n",
    "    user_context_text: str,\n",
    "    history_summary_text: Optional[str],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Render a prompt template using strict placeholder substitution.\n",
    "\n",
    "    Supported placeholders:\n",
    "    - {{preferred_language}}\n",
    "    - {{UserContextText}}\n",
    "    - {{HistorySummaryText}}\n",
    "\n",
    "    No templating engine is used on purpose to keep behavior explicit.\n",
    "    \"\"\"\n",
    "    s = template.replace(\"{{preferred_language}}\", preferred_language or \"th\")\n",
    "    s = s.replace(\"{{UserContextText}}\", user_context_text or \"\")\n",
    "    s = s.replace(\"{{HistorySummaryText}}\", history_summary_text or \"\")\n",
    "    return s\n",
    "\n",
    "# =============================================================================\n",
    "# HyDE output handling\n",
    "# =============================================================================\n",
    "def _extract_hyde_query_texts(hyde_json: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract query_text values from HyDE JSON output.\n",
    "\n",
    "    Expected structure:\n",
    "      {\n",
    "        \"hyde_queries\": [\n",
    "          {\"query_id\": \"...\", \"query_text\": \"...\", ...},\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "\n",
    "    Order is preserved and MUST match embedding row order.\n",
    "    \"\"\"\n",
    "    if not isinstance(hyde_json, dict):\n",
    "        raise ValueError(\"hyde_output must be a dict\")\n",
    "\n",
    "    items = hyde_json.get(\"hyde_queries\") or []\n",
    "    if not isinstance(items, list):\n",
    "        raise ValueError(\"hyde_output.hyde_queries must be a list\")\n",
    "\n",
    "    out: List[str] = []\n",
    "    for i, it in enumerate(items):\n",
    "        if not isinstance(it, dict):\n",
    "            raise ValueError(f\"hyde_output.hyde_queries[{i}] must be an object\")\n",
    "        out.append(str(it.get(\"query_text\") or \"\").strip())\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _l2_normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Row-wise L2 normalization.\n",
    "\n",
    "    Zero rows are left as zero to avoid NaNs.\n",
    "    \"\"\"\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(\"Expected 2D array for row normalization\")\n",
    "\n",
    "    norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    norms[norms == 0.0] = 1.0\n",
    "    return (x / norms).astype(np.float32)\n",
    "\n",
    "\n",
    "def _atomic_save_npy(path: str, arr: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Best-effort atomic .npy write.\n",
    "\n",
    "    Writes to a temp file and renames to avoid partial reads.\n",
    "    \"\"\"\n",
    "    tmp = path + \".tmp.npy\"\n",
    "    np.save(tmp, arr)\n",
    "    os.replace(tmp, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3dd070-559a-48bf-9ab8-ae544e8d82af",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e1bdb-160e-4c33-87cf-c6ec747b561b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569413d-c403-4849-85df-2440a72804fb",
   "metadata": {},
   "source": [
    "### Load resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3d6bdef-e881-48b1-bd2b-ab99371f0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config()\n",
    "out_dir = cfg[\"artifacts\"][\"user_query_bundles_dir\"]\n",
    "\n",
    "bq = DataQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e833aad-0fd9-48ea-9f0e-ebcb49a65b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# students_path = cfg[\"data\"][\"students_path\"]           # 'data/students.csv'\n",
    "# students      = pd.read_csv(students_path)\n",
    "# students\n",
    "students = bq.get_students() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01831b88-4e0a-4a27-a6b1-4a106591d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions_path = cfg[\"data\"][\"interactions_path\"]   # 'data/interactions.csv'\n",
    "# interactions = pd.read_csv(interactions_path)\n",
    "# interactions\n",
    "interactions = bq.get_interactions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a694295-142c-4010-b9f0-e55cf9c3ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeds_path = cfg[\"data\"][\"feeds_path\"]                 # 'data/feeds.jsonl'\n",
    "# feeds_lookup: Dict[str, Dict[str, Any]] = {}\n",
    "# if os.path.exists(feeds_path):\n",
    "#     feeds = read_jsonl(feeds_path)\n",
    "#     feeds_lookup = {\n",
    "#         str(f.get(\"feed_id\")): f\n",
    "#         for f in feeds\n",
    "#         if isinstance(f, dict) and f.get(\"feed_id\") is not None\n",
    "#     }\n",
    "feeds_lookup = bq.get_user_events_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dc93-6e22-4bb9-9a15-aad548031594",
   "metadata": {},
   "source": [
    "### Read HyDE-related configuration once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5f69e31-630a-458a-a778-e4a1f52688ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(history_threshold,recent_k,feed_text_max_chars,include_recent_feeds,query_embedding_model_name) = _read_hyde_config(cfg)\n",
    "expected_dim = int(cfg.get(\"embeddings\", {}).get(\"dim\", 0) or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27510361-1d0b-4a49-8b1a-882b9c2d3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts()\n",
    "if not prompts:\n",
    "    raise ValueError(\"hyde_prompts missing from parameters/prompts.yaml\")\n",
    "client = build_llm_client_from_yaml(\n",
    "    parameters_path=str(PROJECT_ROOT / \"parameters\" / \"parameters.yaml\"),\n",
    "    credentials_path=str(PROJECT_ROOT / \"parameters\" / \"credentials.yaml\"),\n",
    ")\n",
    "query_embedder = GoogleEmbeddingModel(\n",
    "    model_name=query_embedding_model_name,\n",
    "    credentials_path=str(PROJECT_ROOT / \"parameters\" / \"credentials.yaml\"),\n",
    ")\n",
    "now_iso = datetime.now(timezone.utc).replace(microsecond=0).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4869fd2b-df69-4d84-be64-fd6086155294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleEmbeddingModel(model_name='gemini-embedding-001', credentials_path='/code/src/parameters/credentials.yaml', output_dim=768, uniqueness_guard_enabled=True, uniqueness_guard_min_unique_ratio=0.85, uniqueness_guard_round_decimals=8, _client=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eddc10f2-66ef-4d24-85ea-4eaabbab97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8851d412-f1a5-4aee-b166-233abe60862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29T06:36:53Z | INFO | functions.utils.llm_client | LLM call done | attempt=1 | latency=8.326s | in_tokens=816 | out_tokens=351 | model=gemini-2.5-flash | status=ok\n",
      "2026-01-29T06:36:55Z | INFO | pipeline_1_user_hyde | wrote HyDE bundle student_id=stu_p007 events=4 prompt=hyde_a\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Generate one cached bundle per student\n",
    "# ------------------------------------------------------------------\n",
    "logger = get_logger(\"pipeline_1_user_hyde\")\n",
    "for _, row in students.iterrows():\n",
    "    student_row = row.to_dict()     # convert pd -> dict for each row\n",
    "    student_id  = str(student_row.get(\"student_id\",\"\")).strip()\n",
    "    if not student_id or student_id.lower() == \"nan\":\n",
    "        raise ValueError(f\"Invalid student_id in students.csv: {student_row!r}\")\n",
    "    \n",
    "    user_ctx = build_user_context(student_row)\n",
    "    pref_lang = user_ctx.user_context_json.get(\"preferred_language\",\"th\")\n",
    "\n",
    "    user_events = interactions[interactions[\"user_id\"] == student_id]   # <- user event from interaction.csv\n",
    "    num_events  = int(len(user_events))\n",
    "\n",
    "    history_summary_text : Optional[str] = None\n",
    "    #** Crate by combe data for each person student **#\n",
    "    if num_events > 0:\n",
    "        history_summary_text = build_history_summary(\n",
    "            user_events,\n",
    "            preferred_language   = pref_lang,\n",
    "            include_recent_feeds = include_recent_feeds,\n",
    "            recent_k             = recent_k,\n",
    "            feeds_lookup         = feeds_lookup or None,\n",
    "            feed_text_max_chars  = feed_text_max_chars,\n",
    "        )\n",
    "    \n",
    "    prompt_key = choose_hyde_prompt_key(num_events,history_threshold)\n",
    "    template = prompts.get(prompt_key)\n",
    "    if not template:\n",
    "        raise ValueError(f\"Missing prompt '{prompt_key}' in pormpts.yaml\")\n",
    "    prompt = render_prompt(\n",
    "            template=template,\n",
    "            preferred_language=pref_lang,\n",
    "            user_context_text=user_ctx.user_context_text,\n",
    "            history_summary_text=history_summary_text,\n",
    "        )\n",
    "    # ------------------------------------------------------------------\n",
    "    # LLM call (JSON-only)\n",
    "    # ------------------------------------------------------------------\n",
    "    hyde_json = client.generate_json(prompt)\n",
    "    # ------------------------------------------------------------------\n",
    "    # Embed HyDE queries for fast serving\n",
    "    # ------------------------------------------------------------------\n",
    "    hyde_query_texts = _extract_hyde_query_texts(hyde_json)\n",
    "\n",
    "    if hyde_query_texts:\n",
    "        emb = query_embedder.embed_documents(hyde_query_texts)\n",
    "        emb = np.asarray(emb, dtype = np.float32)\n",
    "        if emb.ndim != 2:\n",
    "            raise ValueError(f\"Invalid embedding shape {emb.shape}\")\n",
    "        emb = _l2_normalize_rows(emb)\n",
    "        if expected_dim and emb.shape[1] != expected_dim:\n",
    "            raise ValueError(\n",
    "                f\"Embedding dim mismatch for student = {student_id}:\"\n",
    "                f\"got {emb.shape[1]} expected {expected_dim}\"\n",
    "            )\n",
    "        dim = int(emb.shape[1])\n",
    "    else:\n",
    "        dim = expected_dim or 0\n",
    "        emb = np.zeros((0,dim), dtype=np.float32)\n",
    "\n",
    "    emb_filename = f\"{student_id}_hyde_q_emb.npy\"\n",
    "    emb_path     = os.path.join(out_dir, emb_filename)\n",
    "    _atomic_save_npy(emb_path, emb)\n",
    "    # ---------------------------------------------------\n",
    "    # Persist cached bundle for online serving\n",
    "    # ---------------------------------------------------\n",
    "    bundle: Dict[str, Any] = {\n",
    "        \"bundle_version\"        : \"v2_hyde_embedded_queries\",\n",
    "        \"student_id\"            : student_id,\n",
    "        \"generated_at\"          : now_iso,\n",
    "        \"prompt_key\"            : prompt_key,\n",
    "        \"preferred_language\"    : pref_lang,\n",
    "        \"num_events\"            : num_events,\n",
    "        \"user_context_json\"     : user_ctx.user_context_json,\n",
    "        \"user_context_text\"     : user_ctx.user_context_text,\n",
    "        \"history_summary_text\"  : history_summary_text,\n",
    "        \"hyde_output\"           : hyde_json,\n",
    "        \"hyde_query_embeddings\" : {\n",
    "            \"path\"        : emb_filename,\n",
    "            \"model\"       : query_embedding_model_name,\n",
    "            \"dim\"         : dim,\n",
    "            \"dtype\"       : \"float32\",\n",
    "            \"num_queries\" : int(len(hyde_query_texts)),\n",
    "            \"normalized\"  : True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{student_id}.json\")\n",
    "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle,f,ensure_ascii=False,indent=2)\n",
    "    logger.info(\n",
    "        \"wrote HyDE bundle student_id=%s events=%d prompt=%s\",\n",
    "        student_id,\n",
    "        num_events,\n",
    "        prompt_key,\n",
    "    )\n",
    "    if verbose > 0:\n",
    "        print(_)\n",
    "        print(f\"student_row -> \\n {student_row}\")\n",
    "        print(f\"user_ctx -> \\n {user_ctx}\")\n",
    "        print(f\"user_events -> \\n {user_events}\")\n",
    "        print(f\"promt_key -> {prompt_key}\")\n",
    "        print(f\"hyde_query_texts->{hyde_query_texts}\")\n",
    "        print(f\"emb -> \\n{emb}\")\n",
    "        print(f\"emb_path ->\\n{emb_path}\")\n",
    "        print(f\"bundle->\\n{bundle}\")\n",
    "        print(\"#\"*100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe3577-2a1a-4e08-826f-2948227eba02",
   "metadata": {},
   "source": [
    "### Got embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d9ee5bc-5c42-4f91-82a2-1f92d229f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01371195, -0.01495129,  0.0114938 , ...,  0.05251655,\n",
       "         0.00130554,  0.00109984],\n",
       "       [-0.01202164, -0.01056953,  0.00548899, ...,  0.02600264,\n",
       "         0.00876749,  0.02130553],\n",
       "       [-0.02254788, -0.03349402,  0.0111196 , ...,  0.05043196,\n",
       "         0.01059734,  0.02642387],\n",
       "       [ 0.01462885, -0.01676613,  0.04861221, ...,  0.04482346,\n",
       "         0.01663549,  0.01487698],\n",
       "       [ 0.00693145, -0.02495532, -0.03097113, ...,  0.02536403,\n",
       "         0.01605297,  0.01515156]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
